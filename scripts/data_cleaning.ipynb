{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Data source: https://openlibrary.org/developers/dumps\n",
    "The data dumps from openlibrary.org are huge so to make the project manageable, we limited the content to books on articial intelligence. To accomplish this, all three dump files (ol_dump_works_2019_01-31.txt, ol_dump_editions_2019-01-31.txt, and ol_dump_editions_2019-01-31.txt were downloaded from openlibrary.org, then processed as follows:\n",
    "\n",
    "```\n",
    "grep -i 'artificial\\ intelligence' ol_dump_works_2019_01-31.txt > artificial_intelligence_works.txt\n",
    "grep -Po 'OL[0-9]+A' artificial_intelligence_works.txt | uniq > ai-authors-id.txt\n",
    "grep -f ai-authors.txt ol_dump_editions_2019-01-31.txt >  ai-authors.tsv\n",
    "grep -Po 'OL[0-9]+W' artificial_intelligence_works.txt | uniq > ai-works-id.txt\n",
    "grep -f ai-works-id.txt  ol_dump_editions_2019-01-31.txt > ai-editions.tsv\n",
    "```\n",
    "\n",
    "The code below parses these files and extracts the data needed for the database. \n",
    "\n",
    "### Step 1: Process 'works' data\n",
    "Inputs: \n",
    "../data_output/artificial_intelligence_works.txt\n",
    "\n",
    "Outputs: \n",
    "../data_output/works.csv\n",
    "../data_output/author_works.csv\n",
    "../data_output/works_subject.csv\n",
    "../data_output/subjects.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "doc = \"../data_output/artificial_intelligence_works.txt\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['work_id'] = df.path.str[7:]\n",
    "\n",
    "# create a list containing only the id and json data\n",
    "works = df[['work_id', 'details']].values.tolist()\n",
    "\n",
    "# initialize some lists to hold the data that will get saved in csv files\n",
    "titles = [] # work_id and title\n",
    "work_authors = [] # work_id and author_id\n",
    "subjects = [] # temporary storage for work_id and subject\n",
    "subject_tbl = [] # subject_id \n",
    "work_subject = [] #\n",
    "\n",
    "# Loop through 'works' and pull out the pieces of data we want in the database\n",
    "for work in works:\n",
    "    details = json.loads(work[1])\n",
    "    \n",
    "    # append title to list\n",
    "    titles.append([work[0], details[\"title\"]])\n",
    "    \n",
    "    # append authors to list - not all works have authors!\n",
    "    try:\n",
    "        for a in details[\"authors\"]:\n",
    "            # work_authors.append([works[0], json.dumps(a[\"author\"][\"key\"][9:])])\n",
    "            # author_id = json.dumps(a[\"author\"][\"key\"][9:])\n",
    "            work_authors.append([work[0], a[\"author\"][\"key\"][9:]])\n",
    "    except:\n",
    "        # print(json.dumps(details))\n",
    "        continue\n",
    "        \n",
    "    # append subjects to list\n",
    "    try:\n",
    "        for s in details[\"subjects\"]:\n",
    "            subjects.append([work[0], s])\n",
    "    except:\n",
    "        # print(json.dumps(details))\n",
    "        continue\n",
    "\n",
    "# Create a set from the subjects list  \n",
    "unique_subjects = set()\n",
    "for subject in subjects:\n",
    "    unique_subjects.add(subject[1])\n",
    "    \n",
    "# Generate a subject id for each subject in the set.\n",
    "n = 1\n",
    "for u in sorted(unique_subjects):\n",
    "    subject_tbl.append([str(n), u])\n",
    "    n += 1\n",
    "\n",
    "# Create a dictionary to allow subject lookups\n",
    "subject_id = {}\n",
    "for s in subject_tbl:\n",
    "    subject_id[s[1]] = s[0] \n",
    "\n",
    "# Create a list of lists containing pairs of subject id's and work id's    \n",
    "\n",
    "for s in subjects:\n",
    "    work_subject.append([s[0], subject_id[s[1]]])\n",
    "\n",
    "# Write \"titles\" to csv\n",
    "with open(\"../data_output/works.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(titles)\n",
    "\n",
    "# Write \"work_authors\" to csv    \n",
    "with open(\"../data_output/author_works.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(work_authors)\n",
    "    \n",
    "with open(\"../data_output/work_subjects.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(work_subject)    \n",
    "    \n",
    "with open(\"../data_output/subjects.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(subject_tbl)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process 'authors' data\n",
    "Inputs: \n",
    "../data_output/ai-authors.tsv\n",
    "\n",
    "Outputs:\n",
    "../data_output/authors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"../data_output/ai-authors.tsv\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['author_id'] = df.path.str[9:]\n",
    "\n",
    "# create a list of lists containing only the id and json data\n",
    "authors = df[['author_id', 'details']].values.tolist()\n",
    "\n",
    "author_ids = []\n",
    "\n",
    "for author in authors:\n",
    "    details = json.loads(author[1])\n",
    "    \n",
    "    # append title to list\n",
    "    author_ids.append([author[0], details[\"name\"]])\n",
    "\n",
    "with open(\"../data_output/authors.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(author_ids)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process 'editions' data\n",
    "Inputs: \n",
    "../data_output/ai-editions.tsv\n",
    "\n",
    "Outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"../data_output/ai-editions.tsv\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['edition_id'] = df.path.str[7:]\n",
    "\n",
    "# create a list of lists containing only the id and json data\n",
    "editions_master = df[['edition_id', 'details']].values.tolist()\n",
    "\n",
    "edition_work = [] # edition_id, work_id pairs\n",
    "edition_pages = []\n",
    "edition_isbn10 = []\n",
    "edition_isbn13 = []\n",
    "edition_title = []\n",
    "edition_physical_format = []\n",
    "\n",
    "for edition in editions_master:\n",
    "    details = json.loads(edition[1])\n",
    "    \n",
    "    # get work_ids\n",
    "    edition_work.append([edition[0], details['works'][0]['key'][7:]])\n",
    "    \n",
    "    # get number of pages\n",
    "    if 'number_of_pages' in details.keys():\n",
    "        edition_pages.append([edition[0], details['number_of_pages']])\n",
    "        \n",
    "    # get isbn10\n",
    "    if 'isbn_10' in details.keys():\n",
    "        edition_isbn10.append([edition[0], details['isbn_10']])\n",
    "    \n",
    "    # get isbn13\n",
    "    if 'isbn_13' in details.keys():\n",
    "        edition_isbn13.append([edition[0], details['isbn_13']])\n",
    "\n",
    "    # get title\n",
    "    if 'title' in details.keys():\n",
    "        edition_title.append([edition[0], details['title']])\n",
    "\n",
    "    # get format\n",
    "    if 'physical_format' in detail.keys():\n",
    "        edition_physical_format.append([edition[0], details['physical_format']])\n",
    "        \n",
    "    \n",
    "    \n",
    "#with open(\"../data_output/edition_work.csv\", 'w', encoding='utf-8') as myfile:\n",
    "#    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#    wr.writerows(edition_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editions_master[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(editions_master[700][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
