{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Data source: https://openlibrary.org/developers/dumps\n",
    "The data dumps from openlibrary.org are huge so to make the project manageable, we limited the content to books on articial intelligence. To accomplish this, all three dump files (ol_dump_works_2019_01-31.txt, ol_dump_editions_2019-01-31.txt, and ol_dump_editions_2019-01-31.txt were downloaded from openlibrary.org, then processed as follows:\n",
    "\n",
    "```\n",
    "grep -i 'artificial\\ intelligence' ol_dump_works_2019_01-31.txt > artificial_intelligence_works.txt\n",
    "grep -Po 'OL[0-9]+A' artificial_intelligence_works.txt | uniq > ai-authors-id.txt\n",
    "grep -f ai-authors.txt ol_dump_editions_2019-01-31.txt >  ai-authors.tsv\n",
    "grep -Po 'OL[0-9]+W' artificial_intelligence_works.txt | uniq > ai-works-id.txt\n",
    "grep -f ai-works-id.txt  ol_dump_editions_2019-01-31.txt > ai-editions.tsv\n",
    "```\n",
    "\n",
    "The code below parses these files and extracts the data needed for the database. \n",
    "\n",
    "### Step 1: Process 'works' data\n",
    "**Inputs:**   \n",
    "../data_output/artificial_intelligence_works.txt\n",
    "\n",
    "**Outputs:**   \n",
    "../data_output/works.csv  \n",
    "../data_output/author_works.csv  \n",
    "../data_output/works_subject.csv  \n",
    "../data_output/subjects.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "doc = \"../data_output/artificial_intelligence_works.txt\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['work_id'] = df.path.str[7:]\n",
    "\n",
    "# create a list containing only the id and json data\n",
    "works = df[['work_id', 'details']].values.tolist()\n",
    "\n",
    "# initialize some lists to hold the data that will get saved in csv files\n",
    "titles = [] # work_id and title\n",
    "work_authors = [] # work_id and author_id\n",
    "subjects = [] # temporary storage for work_id and subject\n",
    "subject_tbl = [] # subject_id \n",
    "work_subject = [] #\n",
    "edition_publisher\n",
    "\n",
    "# Loop through 'works' and pull out the pieces of data we want in the database\n",
    "for work in works:\n",
    "    details = json.loads(work[1])\n",
    "    \n",
    "    # append title to list\n",
    "    titles.append([work[0], details[\"title\"]])\n",
    "    \n",
    "    # append authors to list - not all works have authors!\n",
    "    try:\n",
    "        for a in details[\"authors\"]:\n",
    "            # work_authors.append([works[0], json.dumps(a[\"author\"][\"key\"][9:])])\n",
    "            # author_id = json.dumps(a[\"author\"][\"key\"][9:])\n",
    "            work_authors.append([work[0], a[\"author\"][\"key\"][9:]])\n",
    "    except:\n",
    "        # print(json.dumps(details))\n",
    "        continue\n",
    "        \n",
    "    # append subjects to list\n",
    "    try:\n",
    "        for s in details[\"subjects\"]:\n",
    "            subjects.append([work[0], s])\n",
    "    except:\n",
    "        # print(json.dumps(details))\n",
    "        continue\n",
    "\n",
    "# Create a set from the subjects list  \n",
    "unique_subjects = set()\n",
    "for subject in subjects:\n",
    "    unique_subjects.add(subject[1])\n",
    "    \n",
    "# Generate a subject id for each subject in the set.\n",
    "n = 1\n",
    "for u in sorted(unique_subjects):\n",
    "    subject_tbl.append([str(n), u])\n",
    "    n += 1\n",
    "\n",
    "# Create a dictionary to allow subject lookups\n",
    "subject_id = {}\n",
    "for s in subject_tbl:\n",
    "    subject_id[s[1]] = s[0] \n",
    "\n",
    "# Create a list of lists containing pairs of subject id's and work id's    \n",
    "\n",
    "for s in subjects:\n",
    "    work_subject.append([s[0], subject_id[s[1]]])\n",
    "\n",
    "# Write \"titles\" to csv\n",
    "with open(\"../data_output/works.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(titles)\n",
    "\n",
    "# Write \"work_authors\" to csv    \n",
    "with open(\"../data_output/author_works.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(work_authors)\n",
    "    \n",
    "with open(\"../data_output/work_subjects.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(work_subject)    \n",
    "    \n",
    "with open(\"../data_output/subjects.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(subject_tbl)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process 'authors' data\n",
    "**Inputs:**   \n",
    "../data_output/ai-authors.tsv\n",
    "\n",
    "**Outputs:**  \n",
    "../data_output/authors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"../data_output/ai-authors.tsv\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['author_id'] = df.path.str[9:]\n",
    "\n",
    "# create a list of lists containing only the id and json data fields\n",
    "authors = df[['author_id', 'details']].values.tolist()\n",
    "\n",
    "author_ids = []  # initialize a list to hold extracted data\n",
    "# iterate through the list and extract author names from the json data\n",
    "for author in authors:\n",
    "    details = json.loads(author[1])\n",
    "    \n",
    "    # append authour_id and name to list\n",
    "    author_ids.append([author[0], details[\"name\"]])\n",
    "\n",
    "# write author_ids list to a csv file    \n",
    "with open(\"../data_output/authors.csv\", 'w', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(author_ids)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process 'editions' data\n",
    "**Inputs:**   \n",
    "../data_output/ai-editions.tsv\n",
    "\n",
    "**Outputs:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"../data_output/ai-editions.tsv\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['edition_id'] = df.path.str[7:]\n",
    "\n",
    "# create a list of lists containing only the id and json data\n",
    "editions_master = df[['edition_id', 'details']].values.tolist()\n",
    "\n",
    "edition_work = [] # edition_id, work_id pairs\n",
    "edition_pages = []\n",
    "edition_isbn10 = []\n",
    "edition_isbn13 = []\n",
    "edition_title = []\n",
    "edition_physical_format = []\n",
    "edition_publisher = []\n",
    "\n",
    "for edition in editions_master:\n",
    "    details = json.loads(edition[1])\n",
    "    \n",
    "    # get work_ids\n",
    "    edition_work.append([edition[0], details['works'][0]['key'][7:]])\n",
    "    \n",
    "    # get number of pages\n",
    "    if 'number_of_pages' in details.keys():\n",
    "        edition_pages.append([edition[0], details['number_of_pages']])\n",
    "        \n",
    "    # get isbn10\n",
    "    if 'isbn_10' in details.keys():\n",
    "        edition_isbn10.append([edition[0], details['isbn_10']])\n",
    "    \n",
    "    # get isbn13\n",
    "    if 'isbn_13' in details.keys():\n",
    "        edition_isbn13.append([edition[0], details['isbn_13']])\n",
    "\n",
    "    # get title\n",
    "    if 'title' in details.keys():\n",
    "        edition_title.append([edition[0], details['title']])\n",
    "\n",
    "    # get format\n",
    "    if 'physical_format' in details.keys():\n",
    "        edition_physical_format.append([edition[0], details['physical_format']])\n",
    "        \n",
    "    \n",
    "    # get publisher data\n",
    "    pubn = 0\n",
    "    publisher_ids = {}\n",
    "    if 'publishers' in details.keys():\n",
    "        publisher = details['publishers'][0]\n",
    "        if publisher not in publisher_ids.keys():\n",
    "            publisher_ids[publisher] = str(n)\n",
    "            n += 1\n",
    "        if 'publish_places' in details.keys():\n",
    "            pub_place = details['publish_places'][0]\n",
    "        else:\n",
    "            pub_place = \"NULL\"\n",
    "        if 'publish_date' in details.keys():\n",
    "            pub_date = details['publish_date']\n",
    "        else:\n",
    "            pub_date = \"NULL\"\n",
    "        edition_publisher.append([publisher_ids[publisher], publisher, pub_place, pub_date])\n",
    "\n",
    "#with open(\"../data_output/edition_work.csv\", 'w', encoding='utf-8') as myfile:\n",
    "#    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#    wr.writerows(edition_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 'publisher' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "doc = \"../data_output/ai-editions.tsv\"\n",
    "df = pd.read_csv(doc, sep='\\t', header=None)\n",
    "\n",
    "# add column names\n",
    "df.columns = ['type', 'path', 'revisions', 'timestamp', 'details']\n",
    "\n",
    "# get 'work_id' from 'path'\n",
    "df['edition_id'] = df.path.str[7:]\n",
    "\n",
    "# create a list of lists containing only the id and json data\n",
    "editions_master = df[['edition_id', 'details']].values.tolist()\n",
    "edition_publisher = [[\"edition_id\", \"publisher_id\"]]\n",
    "publisher_place = [[\"pub_id\", \"place_id\"]]\n",
    "publisher_ids = {\"NULL\": \"0\"} # pub_name => pub_id\n",
    "place_ids = {\"NULL\": \"0\"} # place_name => place_id\n",
    "\n",
    "pub_id = 1\n",
    "place_id = 1\n",
    "for edition in editions_master:\n",
    "    details = json.loads(edition[1])\n",
    "\n",
    "    if 'publishers' in details.keys():\n",
    "        publisher = details['publishers'][0]\n",
    "        \n",
    "        # create a unique id for publisher\n",
    "        if publisher not in publisher_ids.keys():\n",
    "            publisher_ids[publisher] = str(pub_id)\n",
    "            pub_id += 1\n",
    "\n",
    "        if 'publish_places' in details.keys():\n",
    "            pub_place = details['publish_places'][0]\n",
    "        else:\n",
    "            pub_place = \"NULL\"\n",
    "        \n",
    "        # create a unique id for publisher place\n",
    "        if pub_place not in place_ids.keys():\n",
    "            place_ids[pub_place] = str(place_id)\n",
    "            place_id += 1\n",
    "        \n",
    "        # Move this to edition_tbl\n",
    "        #if 'publish_date' in details.keys():\n",
    "        #    pub_date = details['publish_date']\n",
    "        #else:\n",
    "        #    pub_date = \"NULL\"\n",
    "        \n",
    "        edition_publisher.append([edition[0], publisher_ids[publisher]])\n",
    "        publisher_place.append([publisher_ids[publisher], place_ids[pub_place]])\n",
    "        \n",
    "with open(\"../data_output/edition_publisher.csv\", 'w', encoding='utf-8', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(edition_publisher)\n",
    "with open(\"../data_output/publisher_place.csv\", 'w', encoding='utf-8', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(publisher_place)\n",
    "        \n",
    "# publisher_ids and place_ids are dicts so they need to be handled differently\n",
    "with open(\"../data_output/publishers.csv\", 'w', encoding='utf-8', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['pub_name', 'pub_id'])\n",
    "    for key in publisher_ids.keys():\n",
    "        wr.writerow([key, publisher_ids[key]])\n",
    "        \n",
    "with open(\"../data_output/places.csv\", 'w', encoding='utf-8', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['place_name', 'place_id'])\n",
    "    for key in place_ids.keys():\n",
    "        wr.writerow([key, place_ids[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NULL': '0', 'The MIT Press': '1'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record viewer\n",
    "Change n to look at individual json records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'publishers': ['Springer'],\n",
       " 'physical_format': 'Hardcover',\n",
       " 'first_sentence': {'type': '/type/text',\n",
       "  'value': 'Conceptual Graphs are a flexible, extensible method for knowledge representation.'},\n",
       " 'key': '/books/OL10325063M',\n",
       " 'weight': '13.9 ounces',\n",
       " 'title': 'Reasoning and Unification over Conceptual Graphs',\n",
       " 'number_of_pages': 162,\n",
       " 'isbn_13': ['9780306474873'],\n",
       " 'covers': [2358367],\n",
       " 'edition_name': '1 edition',\n",
       " 'languages': [{'key': '/languages/eng'}],\n",
       " 'isbn_10': ['0306474875'],\n",
       " 'latest_revision': 3,\n",
       " 'last_modified': {'type': '/type/datetime',\n",
       "  'value': '2010-04-13T05:50:23.081550'},\n",
       " 'authors': [{'key': '/authors/OL3419887A'}],\n",
       " 'publish_date': 'January 31, 2003',\n",
       " 'works': [{'key': '/works/OL9383183W'}],\n",
       " 'type': {'key': '/type/edition'},\n",
       " 'subjects': ['Artificial intelligence',\n",
       "  'Computers',\n",
       "  'Computers - General Information',\n",
       "  'Computer Books: General',\n",
       "  'Artificial Intelligence - General',\n",
       "  'Discrete Mathematics',\n",
       "  'Information Theory',\n",
       "  'Computers / Artificial Intelligence',\n",
       "  'Computers / Information Theory',\n",
       "  'Computers : Artificial Intelligence - General',\n",
       "  'Mathematics : Discrete Mathematics',\n",
       "  'Combinatorics',\n",
       "  'Graphic Methods',\n",
       "  'Conceptual structures (Informa',\n",
       "  'Conceptual structures (Information theory)',\n",
       "  'Graph Theory',\n",
       "  'Knowledge representation (Info',\n",
       "  'Knowledge representation (Information theory)'],\n",
       " 'physical_dimensions': '9.5 x 6 x 0.6 inches',\n",
       " 'revision': 3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1\n",
    "json.loads(editions_master[n][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-22-4aea696d7d84>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-4aea696d7d84>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    except IOError:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "csv_columns = ['No','Name','Country']\n",
    "dict_data = [\n",
    "{'No': 1, 'Name': 'Alex', 'Country': 'India'},\n",
    "{'No': 2, 'Name': 'Ben', 'Country': 'USA'},\n",
    "{'No': 3, 'Name': 'Shri Ram', 'Country': 'India'},\n",
    "{'No': 4, 'Name': 'Smith', 'Country': 'USA'},\n",
    "{'No': 5, 'Name': 'Yuva Raj', 'Country': 'India'},\n",
    "]\n",
    "csv_file = \"Foobar.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
